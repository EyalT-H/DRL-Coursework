{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tasks\n",
    "1. Collect Training Data\n",
    "2. Compute Expected Returns\n",
    "3. Actor-Critic Loss Function\n",
    "4. Updating Parameters\n",
    "5. Run Training in a Loop\n",
    "6. Validate\n",
    "7. Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tensorflow.keras import layers\n",
    "from trd_env import trading_env\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Porfolio Value:10000.000696952811; Available Capital: 10000; Current Stocks Held: 1e-06\n",
      "No. Stocks Bought:1e-08; No. Stocks Sold:1e-07; Average Cost:0 \n",
      "Return:100.00000696952812%; 0.0006969528112676926\n",
      "Termination date: 2016-10-31 00:00:00\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"btc_6H_(2016-2018).csv\")\n",
    "df.head()\n",
    "env = trading_env(df)\n",
    "env.reset()\n",
    "env.render()\n",
    "eps = np.finfo(np.float32).eps.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAC(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self,n_actions,n_hl): #number of actions, number of hidden layers employed\n",
    "        super().__init__()\n",
    "        \n",
    "        self.ac = layers.Dense(n_hl, activation = \"relu\")\n",
    "        self.actor = layers.Dense(n_actions) # Number of outputs\n",
    "        self.critic = layers.Dense(1) #Number of outputs\n",
    "        \n",
    "    def call(self,ins):\n",
    "        x = self.ac(ins)\n",
    "        return self.actor(x), self.critic(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_actions = 3\n",
    "n_hl = 50\n",
    "\n",
    "model = SAC(n_actions,n_hl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Collection\n",
    "* During forward pass, use environment state as inputs to generate action probabilities\n",
    "* Critic Value based on the current policy parametized by model weight\n",
    "* Next action sampled using action probabilites generated by the model - applied to the environment generating the next state and reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def walk_env(action):\n",
    "    state,reward,done = env.step(action)\n",
    "    return(np.array(state,np.float32), #might be an issue since our inputs is an array of 6\n",
    "          np.array(reward,np.float32),\n",
    "          np.array(done,np.int32))\n",
    "\n",
    "def tf_env_walk(action):\n",
    "    return tf.numpy_function(walk_env,[action],[tf.float32,tf.float32,tf.float32])\n",
    "\n",
    "def collect(initial_state,model,max_steps):\n",
    "    action_probabilities = tf.TensorArray(dtype = tf.float32, size=0, dynamic_size=True) #all action_probabilites\n",
    "    c_values = tf.TensorArray(dtype = tf.float32, size=0, dynamic_size=True) # all critic_value\n",
    "    rewards = tf.TensorArray(dtype = tf.float32, size=0, dynamic_size=True)\n",
    "    \n",
    "    init_state_shape = initial_state.shape\n",
    "    state = init_state_shape\n",
    "    \n",
    "    for t in tf.range(max_steps):\n",
    "        state = tf.expand_dims(state,0)\n",
    "        action_proba, c_value = model(state)\n",
    "        \n",
    "        #sample next action from the probability\n",
    "        action = tf.random.categorical(action_proba,1)[0,0] #draws sample from categorical distribution (chooses action)\n",
    "        soft_action_proba = tf.nn.softmax(action_proba) # calculates softmax probabilities for each action (calculates the probability of said chosen action)\n",
    "        #store log probability of chosen action\n",
    "        action_probabilities = action_probabilities.write(t,soft_action_proba[0,action])\n",
    "        \n",
    "        #store critic value\n",
    "        c_values = c_values.write(t,tf.squeeze(c_value))\n",
    "        \n",
    "        #Apply action to environment\n",
    "        state,reward,done = tf_env_walk(action)\n",
    "        state.set_shape(init_state_shape)\n",
    "        \n",
    "        #Store reward\n",
    "        rewards = rewards.write(t, reward)\n",
    "        \n",
    "        if tf.cast(done,tf.bool):\n",
    "            break\n",
    "        \n",
    "        action_probabilities = action_probabilities.stack()\n",
    "        c_values = c_values.stack()\n",
    "        rewards = rewards.stack()\n",
    "        \n",
    "        return action_probabilities, c_values, rewards\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Calculate expcted return\n",
    "* Convert reward from each episode into a sequence of expected returns\n",
    "* Sum of rewards taken from t to T and each reward is multiplied by the factor gamma\n",
    "* To stabalise training, returns are standardized using mean and std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_return(rewards,gamma,standardize):\n",
    "    n = tf.shape(rewards)[0]\n",
    "    returns = tf.TensorArray(size = n, dtype=tf.float32)\n",
    "    \n",
    "    #Latest Reward -> First Reward\n",
    "    rewards = tf.cast(rewards[::-1],dtype =tf.float32)\n",
    "    discounted_sum = tf.constant(0.0)\n",
    "    discounted_sum_shape = discounted_sum.shape\n",
    "    for i in tf.range(n):\n",
    "        reward = rewards[i]\n",
    "        discounted_sum = reward * gamma * discounted_sum #Bellman Equation\n",
    "        discounted_sum.set_shape(discounted_sum_shape)\n",
    "        returns = returns.write(i,discounted_sum)\n",
    "    returns = returns.stack()[::-1]\n",
    "    \n",
    "    if standardize:\n",
    "        returns = ((returns-tf.math.reduce_mean(returns))/(tf.math.reduce_std(returns)+eps))\n",
    "        \n",
    "    return returns\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Defining Loss Function\n",
    "* Actor Loss = Bellman Equation*[G-V]\n",
    "* Advantage indicates how much better an action is given a particular state over random actions selected by a policy\n",
    "* Critic Loss: Can be treated as a regression problem using Hubers Loss Function(Less sensetive to outliers relative to squared error loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "huber_loss = tf.keras.losses.Huber(reduction=tf.keras.losses.Reduction.SUM)\n",
    "\n",
    "def compute_loss(action_probabilities, values,returns):\n",
    "    advantage = returns-values\n",
    "    action_lg_prob = tf.math.log(action_probabilities)\n",
    "    actor_loss = -tf.math.reduce_sum(action_lg_prob * advantage)\n",
    "    \n",
    "    critic_loss = huber_loss(values,returns)\n",
    "    return actor_loss + critic_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training and Updating\n",
    "* Runned every episode\n",
    "* Use Adam Optimizer\n",
    "* tf.GradienTape to enable autodiff\n",
    "* Compute sum of undiscounted rewards to meet success criterion\n",
    "* Use tf.function for the training step to improve training speed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate = 0.01)\n",
    "\n",
    "@tf.function\n",
    "def training(init_state,model,optimizer,gamma,max_step_per_episode)-> tf.Tensor:\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        # Run model for one episode to collect data\n",
    "        action_probabilities,values,rewards = collect(init_state,model,max_step_per_episode) # Works\n",
    "        #Calculate expected Returns\n",
    "        returns = exp_return(rewards,gamma) # works\n",
    "        # Convert training data to fit TF shapes\n",
    "        action_probabilities,values,returns = [tf.expand_dims(x,1) for x in [action_probabilities,values,returns]] # Works\n",
    "        #Calculate loss values for the network\n",
    "        loss = compute_loss(action_probabilities, values,returns)#works\n",
    "        \n",
    "    #Compute Loss gradient\n",
    "    grads = tape.gradient(loss,model.trainable_variables)\n",
    "    \n",
    "    #Apply gradient to model params\n",
    "    optimizer.apply_gradients(zip(grads,model.trainable_variables))\n",
    "    \n",
    "    episode_reward = tf.math.reduce_sum(rewards)\n",
    "    \n",
    "    return episode_reward    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/100 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "in user code:\n\n    <ipython-input-8-326441991a1a>:8 training  *\n        action_probabilites,values,rewards = collect(init_state,model,max_step_per_episode)\n    <ipython-input-28-297fec778ebd>:18 collect  *\n        for t in tf.range(max_steps):\n    C:\\Users\\teybo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\autograph\\operators\\control_flow.py:414 for_stmt\n        symbol_names, opts)\n    C:\\Users\\teybo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\autograph\\operators\\control_flow.py:629 _tf_range_for_stmt\n        opts)\n    C:\\Users\\teybo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\autograph\\operators\\control_flow.py:1059 _tf_while_stmt\n        body, get_state, set_state, init_vars, nulls, symbol_names)\n    C:\\Users\\teybo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\autograph\\operators\\control_flow.py:1032 _try_handling_undefineds\n        _verify_loop_init_vars(init_vars, symbol_names, first_iter_vars)\n    C:\\Users\\teybo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\autograph\\operators\\control_flow.py:172 _verify_loop_init_vars\n        'a return statement cannot be placed inside this TensorFlow loop;'\n\n    NotImplementedError: a return statement cannot be placed inside this TensorFlow loop; this may happen if a return statement depends on a static Python condition such as a hyperparameter\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-59-8ab086d985fa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0minitial_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mepisode_reward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgamma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax_step_per_episode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mrunning_reward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mepisode_reward\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m0.01\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mrunning_reward\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m0.99\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xla\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    860\u001b[0m       \u001b[1;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 862\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    863\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    864\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2939\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2940\u001b[0m       (graph_function,\n\u001b[1;32m-> 2941\u001b[1;33m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0m\u001b[0;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3359\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3360\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3361\u001b[1;33m           \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3362\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3204\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3205\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3206\u001b[1;33m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[0;32m   3207\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3208\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    988\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    989\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 990\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    991\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    992\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    632\u001b[0m             \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    633\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 634\u001b[1;33m           \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    635\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    636\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    975\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    976\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 977\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    978\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    979\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: in user code:\n\n    <ipython-input-8-326441991a1a>:8 training  *\n        action_probabilites,values,rewards = collect(init_state,model,max_step_per_episode)\n    <ipython-input-28-297fec778ebd>:18 collect  *\n        for t in tf.range(max_steps):\n    C:\\Users\\teybo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\autograph\\operators\\control_flow.py:414 for_stmt\n        symbol_names, opts)\n    C:\\Users\\teybo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\autograph\\operators\\control_flow.py:629 _tf_range_for_stmt\n        opts)\n    C:\\Users\\teybo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\autograph\\operators\\control_flow.py:1059 _tf_while_stmt\n        body, get_state, set_state, init_vars, nulls, symbol_names)\n    C:\\Users\\teybo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\autograph\\operators\\control_flow.py:1032 _try_handling_undefineds\n        _verify_loop_init_vars(init_vars, symbol_names, first_iter_vars)\n    C:\\Users\\teybo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\autograph\\operators\\control_flow.py:172 _verify_loop_init_vars\n        'a return statement cannot be placed inside this TensorFlow loop;'\n\n    NotImplementedError: a return statement cannot be placed inside this TensorFlow loop; this may happen if a return statement depends on a static Python condition such as a hyperparameter\n"
     ]
    }
   ],
   "source": [
    "max_eps = 100\n",
    "max_step_per_episode = len(df)\n",
    "\n",
    "running_rewarsds = 0\n",
    "reward_threshold = 120000 # Success Criterion > 1000% (return for holding from the start of the market)\n",
    "\n",
    "gamma = 0.99 # Want to maximise future rewards as much as possible\n",
    "\n",
    "with tqdm.trange(max_eps) as t:\n",
    "    for i in t:\n",
    "        initial_state = tf.constant(env.reset(),dtype=tf.float32)\n",
    "        episode_reward = int(training(initial_state,model,optimizer,gamma,max_step_per_episode))\n",
    "        \n",
    "        running_reward = episode_reward * 0.01 + running_reward*0.99\n",
    "        \n",
    "        t.set_description(f\"Epsiode{i}\")\n",
    "        t.set_postfix(episode_reward=episode_reward, running_reward=running_reward)\n",
    "        \n",
    "        #Show average reward every 10 epsiodes\n",
    "        if i % 10 == 0:\n",
    "            print(f\" Episode {i}: Average Reward: {running_reward/i}\")\n",
    "        \n",
    "        if running_reward > reward_threshold:\n",
    "            break\n",
    "    print(f\"Completed: Episode {i}, average_reward:{running_reward/i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_state = tf.constant(env.reset(),dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_probabilities,values,rewards = collect(initial_state,model,max_step_per_episode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "returns = exp_return(rewards,gamma,True)\n",
    "returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_probabilities,values,returns = [tf.expand_dims(x,1) for x in [action_probabilites,values,returns]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = compute_loss(action_probabilities, values,returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Current Issue is with the training function"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
