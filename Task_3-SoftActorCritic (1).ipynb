{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tasks\n",
    "1. Collect Training Data\n",
    "2. Compute Expected Returns\n",
    "3. Actor-Critic Loss Function\n",
    "4. Updating Parameters\n",
    "5. Run Training in a Loop\n",
    "6. Validate\n",
    "7. Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import collections\n",
    "import statistics\n",
    "from tensorflow.keras import layers\n",
    "from trd_env import trading_env\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Porfolio Value:10000.000697066342; Available Capital: 10000; Current Stocks Held: 1e-06\n",
      "No. Stocks Bought:1e-08; No. Stocks Sold:1e-07; Average Cost:0 \n",
      "Return:100.00000697066342%; 0.0006970663416723255\n",
      "Termination date: 2016-10-31 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\teybo\\Anaconda3\\lib\\site-packages\\gym\\logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float16\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"btc_6H_(2016-2018).csv\")\n",
    "df.head()\n",
    "env = trading_env(df)\n",
    "env.reset()\n",
    "env.render()\n",
    "eps = np.finfo(np.float32).eps.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAC(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self,n_actions,n_hl): #number of actions, number of hidden layers employed\n",
    "        super().__init__()\n",
    "        \n",
    "        self.ac = layers.Dense(n_hl, activation = \"relu\")\n",
    "        self.actor = layers.Dense(n_actions) # Number of outputs\n",
    "        self.critic = layers.Dense(1) #Number of outputs\n",
    "        \n",
    "    def call(self,ins):\n",
    "        x = self.ac(ins)\n",
    "        return self.actor(x), self.critic(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_actions = 3\n",
    "n_hl = 50\n",
    "\n",
    "model = SAC(n_actions,n_hl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Collection\n",
    "* During forward pass, use environment state as inputs to generate action probabilities\n",
    "* Critic Value based on the current policy parametized by model weight\n",
    "* Next action sampled using action probabilites generated by the model - applied to the environment generating the next state and reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def walk_env(action):\n",
    "    state,reward,done = env.step(action)\n",
    "    return(np.array(state,np.float32), #might be an issue since our inputs is an array of 6\n",
    "          np.array(reward,np.float32),\n",
    "          np.array(done,np.float32))\n",
    "\n",
    "def tf_env_walk(action):\n",
    "    return tf.numpy_function(walk_env,[action],[tf.float32,tf.float32,tf.float32])\n",
    "\n",
    "def collect(initial_state,model,max_steps):\n",
    "    action_probabilities = tf.TensorArray(dtype = tf.float32, size=0, dynamic_size=True) #all action_probabilites\n",
    "    c_values = tf.TensorArray(dtype = tf.float32, size=0, dynamic_size=True) # all critic_value\n",
    "    rewards = tf.TensorArray(dtype = tf.float32, size=0, dynamic_size=True)\n",
    "    \n",
    "    init_state_shape = initial_state.shape\n",
    "    state = init_state_shape\n",
    "    for t in tf.range(max_steps):\n",
    "        state = tf.expand_dims(4,0)\n",
    "        state=tf.expand_dims(state, axis=-1)\n",
    "        action_proba, c_value = model(state)\n",
    "        \n",
    "        #sample next action from the probability\n",
    "        action = tf.random.categorical(action_proba,1)[0,0] #draws sample from categorical distribution (chooses action)\n",
    "        soft_action_proba = tf.nn.softmax(action_proba) # calculates softmax probabilities for each action (calculates the probability of said chosen action)\n",
    "        #store log probability of chosen action\n",
    "        action_probabilities = action_probabilities.write(t,soft_action_proba[0,action])\n",
    "        \n",
    "        #store critic value\n",
    "        c_values = c_values.write(t,tf.squeeze(c_value))\n",
    "        \n",
    "        #Apply action to environment\n",
    "        state,reward,done = tf_env_walk(action)\n",
    "        state.set_shape(init_state_shape[0])\n",
    "        \n",
    "        #Store reward\n",
    "        rewards = rewards.write(t, reward)\n",
    "        \n",
    "        if tf.cast(done,tf.bool):\n",
    "            break\n",
    "        \n",
    "    action_probabilities = action_probabilities.stack()\n",
    "    c_values = c_values.stack()\n",
    "    rewards = rewards.stack()\n",
    "        \n",
    "    return action_probabilities, c_values, rewards\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "state=env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_probabilities,values,rewards = collect(state,model,3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Calculate expcted return\n",
    "* Convert reward from each episode into a sequence of expected returns\n",
    "* Sum of rewards taken from t to T and each reward is multiplied by the factor gamma\n",
    "* To stabalise training, returns are standardized using mean and std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_return(rewards,gamma,standardize=True):\n",
    "    eps = 0.000001\n",
    "    n = tf.shape(rewards)[0]\n",
    "    returns = tf.TensorArray(size = n, dtype=tf.float32)\n",
    "    \n",
    "    #Latest Reward -> First Reward\n",
    "    rewards = tf.cast(rewards[::-1],dtype =tf.float32)\n",
    "    discounted_sum = tf.constant(0.0)\n",
    "    discounted_sum_shape = discounted_sum.shape\n",
    "    for i in tf.range(n):\n",
    "        reward = rewards[i]\n",
    "        discounted_sum = reward * gamma * discounted_sum #Bellman Equation\n",
    "        discounted_sum.set_shape(discounted_sum_shape)\n",
    "        returns = returns.write(i,discounted_sum)\n",
    "    returns = returns.stack()[::-1]\n",
    "    \n",
    "    if standardize:\n",
    "        returns = ((returns-tf.math.reduce_mean(returns))/(tf.math.reduce_std(returns)+eps))\n",
    "        \n",
    "    return returns\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Defining Loss Function\n",
    "* Actor Loss = Bellman Equation*[G-V]\n",
    "* Advantage indicates how much better an action is given a particular state over random actions selected by a policy\n",
    "* Critic Loss: Can be treated as a regression problem using Hubers Loss Function(Less sensetive to outliers relative to squared error loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "huber_loss = tf.keras.losses.Huber(reduction=tf.keras.losses.Reduction.SUM)\n",
    "\n",
    "def compute_loss(action_probabilities, values,returns):\n",
    "    advantage = returns-values\n",
    "    action_lg_prob = tf.math.log(action_probabilities)\n",
    "    actor_loss = -tf.math.reduce_sum(action_lg_prob * advantage)\n",
    "    \n",
    "    critic_loss = huber_loss(values,returns)\n",
    "    return actor_loss + critic_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training and Updating\n",
    "* Runned every episode\n",
    "* Use Adam Optimizer\n",
    "* tf.GradienTape to enable autodiff\n",
    "* Compute sum of undiscounted rewards to meet success criterion\n",
    "* Use tf.function for the training step to improve training speed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate = 0.01)\n",
    "\n",
    "@tf.function\n",
    "def training(init_state,model,optimizer,gamma,max_step_per_episode)-> tf.Tensor:\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        # Run model for one episode to collect data\n",
    "        action_probabilities,values,rewards = collect(init_state,model,max_step_per_episode) # Works\n",
    "        #Calculate expected Returns\n",
    "        returns = exp_return(rewards,gamma) # works\n",
    "        # Convert training data to fit TF shapes\n",
    "        action_probabilities,values,returns = [tf.expand_dims(x,1) for x in [action_probabilities,values,returns]] # Works\n",
    "        #Calculate loss values for the network\n",
    "        loss = compute_loss(action_probabilities, values,returns)#works\n",
    "        \n",
    "    #Compute Loss gradient\n",
    "    grads = tape.gradient(loss,model.trainable_variables)\n",
    "    \n",
    "    #Apply gradient to model params\n",
    "    optimizer.apply_gradients(zip(grads,model.trainable_variables))\n",
    "    \n",
    "    episode_reward = tf.math.reduce_sum(rewards)\n",
    "    \n",
    "    return episode_reward    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%| | 1/20 [00:06<01:54,  6.05s/it, episode_reward=tf.Tensor(3771648.0, shape=(), dtype=float32), running_reward=tf.T"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Episode 0: Average Reward: inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|▌| 11/20 [01:03<00:48,  5.40s/it, episode_reward=tf.Tensor(1895699.8, shape=(), dtype=float32), running_reward=tf."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Episode 10: Average Reward: 31472.56640625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█| 20/20 [01:54<00:00,  5.74s/it, episode_reward=tf.Tensor(1835248.5, shape=(), dtype=float32), running_reward=tf."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed: Episode 19, average_reward:23772.31640625\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=inf>, <tf.Tensor: shape=(), dtype=float32, numpy=80384.66>, <tf.Tensor: shape=(), dtype=float32, numpy=57467.75>, <tf.Tensor: shape=(), dtype=float32, numpy=48283.3>, <tf.Tensor: shape=(), dtype=float32, numpy=43418.34>, <tf.Tensor: shape=(), dtype=float32, numpy=40500.387>, <tf.Tensor: shape=(), dtype=float32, numpy=38180.113>, <tf.Tensor: shape=(), dtype=float32, numpy=36318.598>, <tf.Tensor: shape=(), dtype=float32, numpy=34781.125>, <tf.Tensor: shape=(), dtype=float32, numpy=33195.137>, <tf.Tensor: shape=(), dtype=float32, numpy=31472.566>, <tf.Tensor: shape=(), dtype=float32, numpy=30337.686>, <tf.Tensor: shape=(), dtype=float32, numpy=29231.053>, <tf.Tensor: shape=(), dtype=float32, numpy=28027.71>, <tf.Tensor: shape=(), dtype=float32, numpy=26928.893>, <tf.Tensor: shape=(), dtype=float32, numpy=26238.742>, <tf.Tensor: shape=(), dtype=float32, numpy=25561.43>, <tf.Tensor: shape=(), dtype=float32, numpy=24900.61>, <tf.Tensor: shape=(), dtype=float32, numpy=24316.584>, <tf.Tensor: shape=(), dtype=float32, numpy=23772.316>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "min_episodes_criterion = 20\n",
    "max_episodes = 100\n",
    "max_steps_per_episode = len(df)-1\n",
    "\n",
    "# Cartpole-v0 is considered solved if average reward is >= 195 over 100 \n",
    "# consecutive trials\n",
    "reward_threshold = 200000\n",
    "running_reward = 0\n",
    "\n",
    "# Discount factor for future rewards\n",
    "gamma = 0.99\n",
    "\n",
    "with tqdm.trange(max_episodes) as t:\n",
    "  for i in t:\n",
    "    initial_state = tf.constant(env.reset(), dtype=tf.float32)\n",
    "    episode_reward = int(training(\n",
    "        initial_state, model, optimizer, gamma, max_steps_per_episode))\n",
    "\n",
    "    episodes_reward.append(episode_reward)\n",
    "    running_reward = statistics.mean(episodes_reward)\n",
    "\n",
    "    t.set_description(f'Episode {i}')\n",
    "    t.set_postfix(\n",
    "        episode_reward=episode_reward, running_reward=running_reward)\n",
    "\n",
    "    # Show average episode reward every 10 episodes\n",
    "    if i % 10 == 0:\n",
    "      pass # print(f'Episode {i}: average reward: {avg_reward}')\n",
    "\n",
    "    if running_reward > reward_threshold and i >= min_episodes_criterion:  \n",
    "        break\n",
    "\n",
    "print(f'\\nSolved at episode {i}: average reward: {running_reward:.2f}!')\n",
    "\n",
    "print(episodes_reward)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
